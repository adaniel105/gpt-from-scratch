{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3Cdb0LuhNQXdjuQcJ6ESK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "6Gkv1yfVKWor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c67558-4f2e-4927-e168-bb8a5f038a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-21 11:10:08--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-08-21 11:10:08 (24.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "X-fWg7aYs6Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of dataset in characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G59FkgzZtKpd",
        "outputId": "9b84b406-933e-4c30-cc22-06dae6f564e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1tz7RdotXxV",
        "outputId": "a446a269-d9e6-46d1-8f27-3065d31919a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_sz = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSaueaAsubt6",
        "outputId": "ea79d362-5f47-4b55-9ddc-30ff5145058b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a simple tokenizer\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i: ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda c: ''.join([itos[i] for i in c])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtwrJ-Pvt06",
        "outputId": "99caee24-8b06-4b6b-a5ed-d992cd4f6479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding entirety of input text with tokenizer\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype= torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWQAaA0oyjYM",
        "outputId": "b39d67e9-3352-4d11-97d0-bdb8e9799682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#90/10 data split\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]\n",
        "\n",
        "print(train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gYaLqdX3onG",
        "outputId": "73839b18-a16c-4cd6-e8f2-85f388c7a03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization of the autoregressive techniuque used by transformers for language modeling\n",
        "block_sz = 8\n",
        "x = train[:block_sz]\n",
        "y = train[1:block_sz + 1]\n",
        "for t in range(block_sz):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG-ZoDk53-aX",
        "outputId": "78edc0f7-db46-4336-c165-294a1f42ed01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(80)\n",
        "batch_sz = 4 #number of input sequences to be processed in parallel\n",
        "block_sz = 8 #maximum context length of predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate a small batch of data of inputs x and targets y\n",
        "  data = train if split == 'train' else val\n",
        "  ix = torch.randint(len(data) - block_sz, (batch_sz,))\n",
        "  x = torch.stack([data[i:i+block_sz] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_sz+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_sz): #batch dimension\n",
        "  for t in range(block_sz): #time dimension\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when the input is {context.tolist()}, the output is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z116RYtz75sa",
        "outputId": "57ade833-4ee0-4353-cb72-371e8635af78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 42,  1, 42, 47, 42,  1, 58],\n",
            "        [39, 42, 50, 63,  1, 57, 54, 53],\n",
            "        [51,  1, 58, 53,  1, 39, 50, 50],\n",
            "        [63, 53, 59,  6,  1, 59, 52, 41]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[42,  1, 42, 47, 42,  1, 58, 56],\n",
            "        [42, 50, 63,  1, 57, 54, 53, 49],\n",
            "        [ 1, 58, 53,  1, 39, 50, 50, 11],\n",
            "        [53, 59,  6,  1, 59, 52, 41, 50]])\n",
            "-----\n",
            "when the input is [52], the output is 42\n",
            "when the input is [52, 42], the output is 1\n",
            "when the input is [52, 42, 1], the output is 42\n",
            "when the input is [52, 42, 1, 42], the output is 47\n",
            "when the input is [52, 42, 1, 42, 47], the output is 42\n",
            "when the input is [52, 42, 1, 42, 47, 42], the output is 1\n",
            "when the input is [52, 42, 1, 42, 47, 42, 1], the output is 58\n",
            "when the input is [52, 42, 1, 42, 47, 42, 1, 58], the output is 56\n",
            "when the input is [39], the output is 42\n",
            "when the input is [39, 42], the output is 50\n",
            "when the input is [39, 42, 50], the output is 63\n",
            "when the input is [39, 42, 50, 63], the output is 1\n",
            "when the input is [39, 42, 50, 63, 1], the output is 57\n",
            "when the input is [39, 42, 50, 63, 1, 57], the output is 54\n",
            "when the input is [39, 42, 50, 63, 1, 57, 54], the output is 53\n",
            "when the input is [39, 42, 50, 63, 1, 57, 54, 53], the output is 49\n",
            "when the input is [51], the output is 1\n",
            "when the input is [51, 1], the output is 58\n",
            "when the input is [51, 1, 58], the output is 53\n",
            "when the input is [51, 1, 58, 53], the output is 1\n",
            "when the input is [51, 1, 58, 53, 1], the output is 39\n",
            "when the input is [51, 1, 58, 53, 1, 39], the output is 50\n",
            "when the input is [51, 1, 58, 53, 1, 39, 50], the output is 50\n",
            "when the input is [51, 1, 58, 53, 1, 39, 50, 50], the output is 11\n",
            "when the input is [63], the output is 53\n",
            "when the input is [63, 53], the output is 59\n",
            "when the input is [63, 53, 59], the output is 6\n",
            "when the input is [63, 53, 59, 6], the output is 1\n",
            "when the input is [63, 53, 59, 6, 1], the output is 59\n",
            "when the input is [63, 53, 59, 6, 1, 59], the output is 52\n",
            "when the input is [63, 53, 59, 6, 1, 59, 52], the output is 41\n",
            "when the input is [63, 53, 59, 6, 1, 59, 52, 41], the output is 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEEDING OUR PREPROCESSED TRAINING DATA INTO NEURAL NETWORKS\n",
        "\n",
        "1. THE BI-GRAM LANGUAGE MODEL\n",
        "    A simple language model that predicts the next token in the sequence based on the single previous token. Can be extended to predict based on n preceding tokens (n-gram model)"
      ],
      "metadata": {
        "id": "548bb9aiMYh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(80)\n",
        "\n",
        "#model hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_sz):\n",
        "      super().__init__()\n",
        "      #each token directly reads off the logits for the next token in a lookup table\n",
        "      self.token_embedding_table = nn.Embedding(vocab_sz, vocab_sz)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      #both idx and tensor are 2-dimensional (B,T) tensor of integers\n",
        "      logits = self.token_embedding_table(idx) #(B, T, C)\n",
        "\n",
        "      if targets == None:\n",
        "        loss = None\n",
        "      else:\n",
        "        #since pytorch expects a (B,C,T) tensor, we reshape the tensor using torch.view()\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T) #combining our batch and time dimensions\n",
        "\n",
        "        loss = F.cross_entropy(logits, targets) #negative log-likehood loss fn\n",
        "      return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "        # we pass our indexes to the forward fn to get our predictions(logits)\n",
        "        logits , loss = self(idx)\n",
        "        # focus on only the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B,C)\n",
        "        #convert predictions to class probabilities\n",
        "        probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "        # sample from the prob. distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        # append sampled index to running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_sz)\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits)\n",
        "print(logits.shape)\n",
        "print(loss) #NOTE: ideal nll loss = -ln(1/vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih2olNgIAeta",
        "outputId": "c270896b-cd7c-487e-8ae8-b47902ebbab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2811, -0.2446,  0.3116,  ...,  0.2522, -0.0340, -0.8544],\n",
            "        [-0.0927, -1.2656, -2.1652,  ..., -0.0186,  0.9552,  1.3387],\n",
            "        [-0.3464, -1.1768, -2.6356,  ...,  1.7470,  1.1502, -0.0674],\n",
            "        ...,\n",
            "        [ 1.0463, -0.3701,  1.5730,  ...,  0.1144,  1.5161,  1.6939],\n",
            "        [ 1.2811, -0.2446,  0.3116,  ...,  0.2522, -0.0340, -0.8544],\n",
            "        [ 0.9166, -0.5317, -0.2654,  ...,  0.3093, -0.0615, -0.7909]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([32, 65])\n",
            "tensor(4.7332, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_seq = m.generate(torch.zeros((1,1), dtype= torch.long), max_new_tokens=100)[0].tolist() #generate a sequence of 100 characters using a newline character as initial idx\n",
        "print(decode(char_seq)) #prev op returns vaiid tokenizer idxs which we then decode"
      ],
      "metadata": {
        "id": "PJKVfuDctprx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c0840e-1d51-4b54-fd35-7f4e9077201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-HZIWfB&kDAB-H3';bfsvtu'zaXk'zLxKtzdJRo3$B!VWmk3jLRDAvOYi&JGNHaeb:k-j-vjF\n",
            "muNamb:plt?;ccECIifWvFJl$F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create our optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "D86whd3iBjQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, y = get_batch(split)\n",
        "      logits, loss = model(X,y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "t7iKmP-TVTFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "  logits , loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdrUb4LwYN83",
        "outputId": "486c74cd-e1e9-47f3-f0f2-190846cc6251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5787, val loss 4.5676\n",
            "step 300: train loss 4.3431, val loss 4.3617\n",
            "step 600: train loss 4.1470, val loss 4.1720\n",
            "step 900: train loss 3.9767, val loss 3.9886\n",
            "step 1200: train loss 3.8490, val loss 3.8423\n",
            "step 1500: train loss 3.6680, val loss 3.6904\n",
            "step 1800: train loss 3.5527, val loss 3.5574\n",
            "step 2100: train loss 3.4283, val loss 3.4135\n",
            "step 2400: train loss 3.3299, val loss 3.3384\n",
            "step 2700: train loss 3.2359, val loss 3.2498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87kktiiZb4W",
        "outputId": "60297ad5-933e-4565-8d38-858b6229dc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mjCdzK&ncr hyocsiEkio,HO&qse,\n",
            "mbfWhQrdomiQW-G,s.\n",
            "CE\n",
            "Ty!f&\n",
            "Apr?\n",
            "Tw$XHmSGbrixfYo r'JAumL:,\n",
            "FJxJmbldyproprf--e.:tr.\n",
            "tob laRWxRbco,dzdy-apNUproGHAy. b;Ho:th?\n",
            "Abl,lkO:'Lteasher?qXB$P?WxK3F&nty:UXqQwgF!KKh,idrfZ wpRABxJYdGHBeRhRSQWWwWYs;zG;x:\n",
            "D:;p fuAQwhyS'MPT?x\n",
            "F wlurafB:GCQ,whidsf?zGA By,\n",
            "B;3xcorCmntv$TCmy.gB?IpjalTCQj,PSwofof:\n",
            "fNNQfWPMdYIn n.gs:\n",
            "Tyt roy&TYQHxjbloOX'hy VoaiYifBqYilyt,\n",
            "TGZMg mureJZ'zAvns?EYbEDxxKBWuieaF.\n",
            "Bcer'QdreubQTe'$ra!gq,HF.\n",
            "zYYmblPfiJOst DmI's hloraviYCHEdomeL\n",
            "Ats-VKub frljjaK$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The Transformer model\n",
        "\n",
        "THE MATHEMATICAL TRICK IN SELF-ATTENTION"
      ],
      "metadata": {
        "id": "1ZGv94i0cKnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(32)\n",
        "B,T, C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IesEXHccKQv",
        "outputId": "17bea765-1912-4885-8c23-d4ca3ab493e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to somehow use aggregate info from prevoius tokens in the past to predict the present while disregarding future tokens. We can do this in a number of ways\n",
        "# 1. x[b,t] = mean_{i<=t} x[b,i]\n",
        "\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]\n",
        "    xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "99qOf1APkoVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZircwq7x4LS",
        "outputId": "97854c90-d0b9-4704-97dc-1d7fd2fc2ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [ 0.9803, -1.7387],\n",
              "        [ 0.3530, -0.2026],\n",
              "        [-0.5071,  0.2125],\n",
              "        [-1.0660,  0.7092],\n",
              "        [-0.3287, -0.2406],\n",
              "        [-0.3393, -1.3003],\n",
              "        [-1.3534,  0.0349]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byx5Y6Amx1Xa",
        "outputId": "93d61d1d-c952-4c73-ba4b-5dc456174e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [-0.3806, -1.0511],\n",
              "        [-0.1361, -0.7683],\n",
              "        [-0.2288, -0.5231],\n",
              "        [-0.3963, -0.2766],\n",
              "        [-0.3850, -0.2706],\n",
              "        [-0.3785, -0.4177],\n",
              "        [-0.5003, -0.3612]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. we manipulate the output of matmul by modifying the multiplying matrix a in such a way the output t+1 is a average of the preceding outputs t\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a,1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHUZPaepmNCH",
        "outputId": "66aa1a29-be95-4f36-ef0c-b5cddd46e706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#therefore\n",
        "w = torch.tril(torch.ones(T,T))\n",
        "w = w / w.sum(1, keepdim=True)\n",
        "xbow2 = w @ x # (B(auto-created by torch for matmul), T, T) @ (B, T, C) ---> (B,T,C)\n",
        "xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkIV2u2uu27R",
        "outputId": "3f232b39-6708-45da-b6a7-bcfcced117f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [-0.3806, -1.0511],\n",
              "        [-0.1361, -0.7683],\n",
              "        [-0.2288, -0.5231],\n",
              "        [-0.3963, -0.2766],\n",
              "        [-0.3850, -0.2706],\n",
              "        [-0.3785, -0.4177],\n",
              "        [-0.5003, -0.3612]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. using softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "w = torch.zeros((T,T))\n",
        "w = w.masked_fill(tril == 0, float(\"-inf\"))\n",
        "w = F.softmax(w, dim=-1)\n",
        "xbow3 = w @ x\n",
        "xbow3[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1InHgL-LyMqz",
        "outputId": "4b1c6351-3e8f-44ce-ee58-6217e6e036e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [-0.3806, -1.0511],\n",
              "        [-0.1361, -0.7683],\n",
              "        [-0.2288, -0.5231],\n",
              "        [-0.3963, -0.2766],\n",
              "        [-0.3850, -0.2706],\n",
              "        [-0.3785, -0.4177],\n",
              "        [-0.5003, -0.3612]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Self-attention!\n",
        "torch.manual_seed(80)\n",
        "head_size = 16\n",
        "B,T,C = 4,8,32\n",
        "x.randn(B,T,C)\n",
        "\n",
        "#self-attention using a single head\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias= False)\n",
        "k = key(x) #(B,T,16) stores some information content about the token\n",
        "q = query(x) # stores information context of the token\n",
        "v = value(x)\n",
        "\n",
        "# matmul of q and k produce attention weights which represent the degree of affinity (similarity) between input tokens\n",
        "weights = q @ k.transpose(-2, -1) # (B, T, 16) * (B, 16, T) --> (B, T, T)\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "out = weights * v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "TZZroHCzbUUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}