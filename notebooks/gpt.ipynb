{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2mR2YbeY1eecrlOFY83KJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LANGUAGE MODELING AND TEXT GENERATION WITH GPT"
      ],
      "metadata": {
        "id": "tZwhCd8VW3av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "6Gkv1yfVKWor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fda03a4-996f-4cfe-adda-c1e2601d3473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-23 15:09:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-08-23 15:09:43 (129 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "X-fWg7aYs6Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of dataset in characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G59FkgzZtKpd",
        "outputId": "bb950b46-5abc-486b-fe9c-57424128aeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1tz7RdotXxV",
        "outputId": "382c1dbe-273e-4531-be87-0127131d1660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_sz = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSaueaAsubt6",
        "outputId": "b398fe82-1dfa-4e5d-d82d-0edeaa83d00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a simple tokenizer\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i: ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda c: ''.join([itos[i] for i in c])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtwrJ-Pvt06",
        "outputId": "dc395c65-6f57-4ce7-a0b1-7436ee6e0f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding entirety of input text with tokenizer\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = torch.tensor(encode(text), dtype= torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWQAaA0oyjYM",
        "outputId": "ae0ead15-aef6-4936-f7cd-909e6406ad37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#90/10 data split\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]\n",
        "\n",
        "print(train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gYaLqdX3onG",
        "outputId": "3e80e96d-390b-49de-dcd6-e89ded652cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization of the autoregressive techniuque used by transformers for language modeling\n",
        "block_sz = 8\n",
        "x = train[:block_sz]\n",
        "y = train[1:block_sz + 1]\n",
        "for t in range(block_sz):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG-ZoDk53-aX",
        "outputId": "7c2e5ba1-87f4-4abd-a0e1-08001068bca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(80)\n",
        "batch_sz = 4 #number of input sequences to be processed in parallel\n",
        "block_sz = 8 #maximum context length of predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate a small batch of data of inputs x and targets y\n",
        "  data = train if split == 'train' else val\n",
        "  ix = torch.randint(len(data) - block_sz, (batch_sz,))\n",
        "  x = torch.stack([data[i:i+block_sz] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_sz+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_sz): #batch dimension\n",
        "  for t in range(block_sz): #time dimension\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when the input is {context.tolist()}, the output is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z116RYtz75sa",
        "outputId": "aa6f9ebc-dcd4-47ba-a82a-16b24e9302fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 42,  1, 42, 47, 42,  1, 58],\n",
            "        [39, 42, 50, 63,  1, 57, 54, 53],\n",
            "        [51,  1, 58, 53,  1, 39, 50, 50],\n",
            "        [63, 53, 59,  6,  1, 59, 52, 41]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[42,  1, 42, 47, 42,  1, 58, 56],\n",
            "        [42, 50, 63,  1, 57, 54, 53, 49],\n",
            "        [ 1, 58, 53,  1, 39, 50, 50, 11],\n",
            "        [53, 59,  6,  1, 59, 52, 41, 50]], device='cuda:0')\n",
            "-----\n",
            "when the input is [52], the output is 42\n",
            "when the input is [52, 42], the output is 1\n",
            "when the input is [52, 42, 1], the output is 42\n",
            "when the input is [52, 42, 1, 42], the output is 47\n",
            "when the input is [52, 42, 1, 42, 47], the output is 42\n",
            "when the input is [52, 42, 1, 42, 47, 42], the output is 1\n",
            "when the input is [52, 42, 1, 42, 47, 42, 1], the output is 58\n",
            "when the input is [52, 42, 1, 42, 47, 42, 1, 58], the output is 56\n",
            "when the input is [39], the output is 42\n",
            "when the input is [39, 42], the output is 50\n",
            "when the input is [39, 42, 50], the output is 63\n",
            "when the input is [39, 42, 50, 63], the output is 1\n",
            "when the input is [39, 42, 50, 63, 1], the output is 57\n",
            "when the input is [39, 42, 50, 63, 1, 57], the output is 54\n",
            "when the input is [39, 42, 50, 63, 1, 57, 54], the output is 53\n",
            "when the input is [39, 42, 50, 63, 1, 57, 54, 53], the output is 49\n",
            "when the input is [51], the output is 1\n",
            "when the input is [51, 1], the output is 58\n",
            "when the input is [51, 1, 58], the output is 53\n",
            "when the input is [51, 1, 58, 53], the output is 1\n",
            "when the input is [51, 1, 58, 53, 1], the output is 39\n",
            "when the input is [51, 1, 58, 53, 1, 39], the output is 50\n",
            "when the input is [51, 1, 58, 53, 1, 39, 50], the output is 50\n",
            "when the input is [51, 1, 58, 53, 1, 39, 50, 50], the output is 11\n",
            "when the input is [63], the output is 53\n",
            "when the input is [63, 53], the output is 59\n",
            "when the input is [63, 53, 59], the output is 6\n",
            "when the input is [63, 53, 59, 6], the output is 1\n",
            "when the input is [63, 53, 59, 6, 1], the output is 59\n",
            "when the input is [63, 53, 59, 6, 1, 59], the output is 52\n",
            "when the input is [63, 53, 59, 6, 1, 59, 52], the output is 41\n",
            "when the input is [63, 53, 59, 6, 1, 59, 52, 41], the output is 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEEDING OUR PREPROCESSED TRAINING DATA INTO NEURAL NETWORKS\n",
        "\n",
        "1. THE BI-GRAM LANGUAGE MODEL\n",
        "    A simple language model that predicts the next token in the sequence based on the single previous token. Can be extended to predict based on n preceding tokens (n-gram model)"
      ],
      "metadata": {
        "id": "548bb9aiMYh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(80)\n",
        "\n",
        "#model hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_sz):\n",
        "      super().__init__()\n",
        "      #each token directly reads off the logits for the next token in a lookup table\n",
        "      self.token_embedding_table = nn.Embedding(vocab_sz, vocab_sz)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      #both idx and tensor are 2-dimensional (B,T) tensor of integers\n",
        "      logits = self.token_embedding_table(idx) #(B, T, C)\n",
        "\n",
        "      if targets == None:\n",
        "        loss = None\n",
        "      else:\n",
        "        #since pytorch expects a (B,C,T) tensor, we reshape the tensor using torch.view()\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T) #combining our batch and time dimensions\n",
        "\n",
        "        loss = F.cross_entropy(logits, targets) #negative log-likehood loss fn\n",
        "      return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "        # we pass our indexes to the forward fn to get our predictions(logits)\n",
        "        logits , loss = self(idx)\n",
        "        # focus on only the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B,C)\n",
        "        #convert predictions to class probabilities\n",
        "        probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "        # sample from the prob. distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        # append sampled index to running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_sz)\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits)\n",
        "print(logits.shape)\n",
        "print(loss) #NOTE: ideal nll loss = -ln(1/vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih2olNgIAeta",
        "outputId": "c6a85b56-7f23-4c85-fc92-21f39447da5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2811, -0.2446,  0.3116,  ...,  0.2522, -0.0340, -0.8544],\n",
            "        [-0.0927, -1.2656, -2.1652,  ..., -0.0186,  0.9552,  1.3387],\n",
            "        [-0.3464, -1.1768, -2.6356,  ...,  1.7470,  1.1502, -0.0674],\n",
            "        ...,\n",
            "        [ 1.0463, -0.3701,  1.5730,  ...,  0.1144,  1.5161,  1.6939],\n",
            "        [ 1.2811, -0.2446,  0.3116,  ...,  0.2522, -0.0340, -0.8544],\n",
            "        [ 0.9166, -0.5317, -0.2654,  ...,  0.3093, -0.0615, -0.7909]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "torch.Size([32, 65])\n",
            "tensor(4.7332, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_seq = m.generate(torch.zeros((1,1), dtype= torch.long, device=device), max_new_tokens=100)[0].tolist() #generate a sequence of 100 characters using a newline character as initial idx\n",
        "print(decode(char_seq)) #prev op returns vaiid tokenizer idxs which we then decode"
      ],
      "metadata": {
        "id": "PJKVfuDctprx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae66ae85-3ddf-4502-c06e-3f779c913195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "zZT.Or:,Fk xfWv$hUNy&rz\n",
            "XdR\n",
            "eyIQfX\n",
            "m,PqRTTZgFXTGyYcWW.zz\n",
            "lrP3Ek&nX\n",
            "M:o'GG.U&bl;YV.:priXCIxi.:NJADm&Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, y = get_batch(split)\n",
        "      logits, loss = model(X,y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "t7iKmP-TVTFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create our optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "D86whd3iBjQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "  logits , loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdrUb4LwYN83",
        "outputId": "767f0d00-dedb-430b-8e07-3afeb2bb016d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5679, val loss 4.5684\n",
            "step 300: train loss 3.1709, val loss 3.1705\n",
            "step 600: train loss 2.7391, val loss 2.7489\n",
            "step 900: train loss 2.6082, val loss 2.6197\n",
            "step 1200: train loss 2.5575, val loss 2.5715\n",
            "step 1500: train loss 2.5117, val loss 2.5468\n",
            "step 1800: train loss 2.4863, val loss 2.5187\n",
            "step 2100: train loss 2.4889, val loss 2.5019\n",
            "step 2400: train loss 2.5030, val loss 2.5177\n",
            "step 2700: train loss 2.4793, val loss 2.4975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype= torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87kktiiZb4W",
        "outputId": "22394a9a-5b80-4315-ca91-412d1e0bdaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INo an ppr y lyoug. S:\n",
            "NI r sthe he bes, h\n",
            "Ormand mend hincly,\n",
            "prthod afot dotors sl bethred,\n",
            "\n",
            "\n",
            "\n",
            "wiste hicint att y,coresld, t courshowaselve J&hy, phowis area, e pssayofo he hout;\n",
            "JUKELE:\n",
            "BRCK:rist iranoowinds TUS:\n",
            "Fowo d I indurdge, su dsuner f t d wnder hin?\n",
            "AShorin thot! wey m l n Noy OLLxayene se chatrilRELIIOREENENVe osinthor s s; it f ded gealor watiblin\n",
            "UNow 'Manuge e ay WAD$Tod nt.\n",
            "BRUSadenddend id t whe itha t?Be dds oungoure therme t or am\n",
            "\n",
            "\n",
            "Bure cam coumaus pt berghe shighang f hephe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The Transformer model\n",
        "\n",
        "THE MATHEMATICAL TRICK IN SELF-ATTENTION"
      ],
      "metadata": {
        "id": "1ZGv94i0cKnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(32)\n",
        "B,T, C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IesEXHccKQv",
        "outputId": "0a9bf628-4d0d-4eef-91cc-5fb4d02822e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to somehow use aggregate info from prevoius tokens in the past to predict the present while disregarding future tokens. We can do this in a number of ways."
      ],
      "metadata": {
        "id": "NxN4MmpPd1Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. x[b,t] = mean_{i<=t} x[b,i]\n",
        "\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]\n",
        "    xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "99qOf1APkoVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZircwq7x4LS",
        "outputId": "37229ad5-89fe-41e3-e6a4-fed120416e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [ 0.9803, -1.7387],\n",
              "        [ 0.3530, -0.2026],\n",
              "        [-0.5071,  0.2125],\n",
              "        [-1.0660,  0.7092],\n",
              "        [-0.3287, -0.2406],\n",
              "        [-0.3393, -1.3003],\n",
              "        [-1.3534,  0.0349]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byx5Y6Amx1Xa",
        "outputId": "ded262da-f222-422b-d702-f77c8c37cf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [-0.3806, -1.0511],\n",
              "        [-0.1361, -0.7683],\n",
              "        [-0.2288, -0.5231],\n",
              "        [-0.3963, -0.2766],\n",
              "        [-0.3850, -0.2706],\n",
              "        [-0.3785, -0.4177],\n",
              "        [-0.5003, -0.3612]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. we manipulate the output of matmul by modifying the multiplying matrix a in such a way the output t+1 is a average of the preceding outputs t\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a,1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHUZPaepmNCH",
        "outputId": "f96db320-dceb-43d0-d665-a5fe1bfb3cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#therefore\n",
        "w = torch.tril(torch.ones(T,T))\n",
        "w = w / w.sum(1, keepdim=True)\n",
        "xbow2 = w @ x # (B(auto-created by torch for matmul), T, T) @ (B, T, C) ---> (B,T,C)\n",
        "xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkIV2u2uu27R",
        "outputId": "e5c28a45-8c30-4c68-9adc-eee62a0149be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [-0.3806, -1.0511],\n",
              "        [-0.1361, -0.7683],\n",
              "        [-0.2288, -0.5231],\n",
              "        [-0.3963, -0.2766],\n",
              "        [-0.3850, -0.2706],\n",
              "        [-0.3785, -0.4177],\n",
              "        [-0.5003, -0.3612]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. using softmax (multicategory equivalent of the sigmoid fn)\n",
        "t_tril = torch.tril(torch.ones(T,T))\n",
        "t_w = torch.zeros((T,T))\n",
        "t_w = t_w.masked_fill(t_tril == 0, float(\"-inf\"))\n",
        "t_w = F.softmax(w, dim=-1)\n",
        "xbow3 = t_w @ x\n",
        "xbow3[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1InHgL-LyMqz",
        "outputId": "3f2ec823-947a-479f-fb0e-9e68abb1b744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7416, -0.3635],\n",
              "        [-0.3806, -1.0511],\n",
              "        [-0.1361, -0.7683],\n",
              "        [-0.2288, -0.5231],\n",
              "        [-0.3963, -0.2766],\n",
              "        [-0.3850, -0.2706],\n",
              "        [-0.3785, -0.4177],\n",
              "        [-0.5003, -0.3612]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Self-attention!\n",
        "torch.manual_seed(80)\n",
        "head_size = 16\n",
        "sa_B,sa_T,sa_C = 4,8,32\n",
        "x= torch.randn(sa_B, sa_T, sa_C)\n",
        "\n",
        "#self-attention using a single head\n",
        "sa_key = nn.Linear(sa_C, head_size, bias=False)\n",
        "sa_query = nn.Linear(sa_C, head_size, bias=False)\n",
        "sa_value = nn.Linear(sa_C, head_size, bias= False)\n",
        "sa_k = sa_key(x) #(B,T,16) stores some information content about the token\n",
        "sa_q = sa_query(x) # stores information context of the token\n",
        "sa_v = sa_value(x)\n",
        "\n",
        "# matmul of q and k produce attention scores which represent the degree of affinity (similarity) between input tokens\n",
        "weights = sa_q @ sa_k.transpose(-2, -1) # (B, T, 16) * (B, 16, T) --> (B, T, T)\n",
        "tril = torch.t_tril(torch.ones(T,T))\n",
        "weights = weights.masked_fill(t_tril== 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "out = weights @ sa_v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "TZZroHCzbUUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ecaa52-a01a-4e8f-c2db-255aca013e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING OUR TRANSFORMER MODEL"
      ],
      "metadata": {
        "id": "5u5UI_8iY-ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9JC4XSkI0M7",
        "outputId": "6d5396d5-c3cb-4e9b-85aa-c0d7e1749ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-23 16:48:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-08-23 16:48:29 (27.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#hyperparameters\n",
        "batch_sz = 64\n",
        "block_sz = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_sz = len(chars)\n",
        "\n",
        "# creating a simple tokenizer\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [\n",
        "    stoi[c] for c in s\n",
        "]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join(\n",
        "    [itos[i] for i in l]\n",
        ")  # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "# 90/10 data split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train if split == 'train' else val\n",
        "    ix = torch.randint(len(data) - block_sz, (batch_sz,))\n",
        "    x = torch.stack([data[i:i+block_sz] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_sz+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, y = get_batch(split)\n",
        "      logits, loss = model(X,y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" implementation of one head of self-attention\"\"\"\n",
        "  def __init__(self, head_sz):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_sz, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_sz, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_sz, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_sz, block_sz)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    W = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "    W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    W = F.softmax(W, dim=-1)\n",
        "    W = self.dropout(W)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = W @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel\"\"\"\n",
        "  def __init__(self, head_sz, num_heads):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_sz) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(head_sz * num_heads, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    proj = self.proj(x)\n",
        "    out = self.dropout(proj)\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" MLP layer with dropout \"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_embd, 4 * n_embd), #attention paper uses 1:4 input to inner-layer dimensionality\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * n_embd, n_embd),\n",
        "      nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_sz = n_embd // n_head\n",
        "    self.s_attn = MultiHeadAttention(head_sz, n_head)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.s_attn(self.ln1(x)) #skip connections as our nn gets more dense\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    \"\"\" Putting it together \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_sz, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_sz, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_sz)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_sz:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "#training loop\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1: #at intervals of 500 or at 4999\n",
        "    losses = estimate_loss()\n",
        "\n",
        "    print(f\"step {iter} : training loss {losses['train']:.4f}, validation loss: {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "RxM5luy9R3oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbcac4c-29ec-4654-8f43-c3769498c12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0 : training loss 4.2684, validation loss: 4.2663\n",
            "step 500 : training loss 2.4637, validation loss: 2.4896\n",
            "step 1000 : training loss 2.4593, validation loss: 2.4842\n",
            "step 1500 : training loss 2.4584, validation loss: 2.4857\n",
            "step 2000 : training loss 2.4587, validation loss: 2.4812\n",
            "step 2500 : training loss 2.4564, validation loss: 2.4877\n",
            "step 3000 : training loss 2.4586, validation loss: 2.4885\n",
            "step 3500 : training loss 2.4566, validation loss: 2.4865\n",
            "step 4000 : training loss 2.4574, validation loss: 2.4872\n",
            "step 4500 : training loss 2.4567, validation loss: 2.4835\n",
            "step 4999 : training loss 2.4569, validation loss: 2.4835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype= torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
        "# we can see our generated text start to mimic the structure of our input proses, and we can even spot some actual english words in there!"
      ],
      "metadata": {
        "id": "kvNWPwEpOinu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43e1e59-aefd-486b-e47c-df17e150c4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bu, athisen:\n",
            "PAno chou.\n",
            "DUSin:\n",
            "Sit alltltaroun e oomeat buteantou m ndsen\n",
            "Be s! an meas d feneesen:\n",
            "\n",
            "Fr,\n",
            "CIUENDWa-widy st tot pl hor gu beg. gutepurse,\n",
            "Anan!\n",
            "Sefus tord.\n",
            "Sitast ntheth g f mefe ns athurerie sathawisul per s cheer aran Whe cllorehalay mot ms dd, tha, y dsaicare pr, st, se f,\n",
            "HEd I at!\n",
            "\n",
            "\n",
            "w tefatiete be brg I owoul fritsting\n",
            "The.\n",
            "Yorky rinsiss t s dyowhave k, laramat\n",
            "TI'd atugr DULot Ifestom cit prdere Whad l hare ghes mpa co o urour tlaingmp, ill by athis, verears.\n",
            "N nd e thereesheifofrewe h telodis.\n",
            "by, de fe par'Or nd mound t.\n",
            "ANRDWAncr heca ft d I anon thellas whithomer stuky thes; cot wickist che by frlidain w,\n",
            "Sofrewerears o y thy y ry hme\n",
            "UMiath, bed-rer y we ary,\n",
            "I ce atrth fave.\n",
            "T:\n",
            "Siso, us, ow f I theed garig theae!\n",
            "Tond,\n",
            "Anavend friean\n",
            "Thernd oust.\n",
            "CE:\n",
            "OUCyou w sesurare t\n",
            "Mam deatotoomy I ofeat d! han! mbu haslfo we se at y ats;\n",
            "He be'd\n",
            "T: thimaith t?\n",
            "Lildsef me?\n",
            "IORithileise touristhag tee at hoth an w thiven hemosten w g.\n",
            "Herithif mamyord. ik, uet y ied he'to \n"
          ]
        }
      ]
    }
  ]
}