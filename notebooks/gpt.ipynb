{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkrKh4HTTAzoUByaC1vqJg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "6Gkv1yfVKWor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473783b5-a8bb-4193-d386-c88f0ef91f7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-21 08:10:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-08-21 08:10:38 (15.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "X-fWg7aYs6Ly"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of dataset in characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G59FkgzZtKpd",
        "outputId": "e2626be2-5bc4-4218-f6b5-5a3e88784f28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1tz7RdotXxV",
        "outputId": "7cae0cb0-b291-4c3f-d0e1-8d4f8d3d2187"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_sz = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSaueaAsubt6",
        "outputId": "9d66811e-2dae-4fc7-e760-49b2012d84b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a simple tokenizer\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i: ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda c: ''.join([itos[i] for i in c])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtwrJ-Pvt06",
        "outputId": "9d3d3d6c-e38b-4c51-fa93-f660785452c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding entirety of input text with tokenizer\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype= torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWQAaA0oyjYM",
        "outputId": "7c0c9eb8-4492-4170-9ef9-d80b654b4eeb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#90/10 data split\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]\n",
        "\n",
        "print(train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gYaLqdX3onG",
        "outputId": "a7919ed9-d734-484f-cef4-23208baa6497"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization of the autoregressive techniuque used by transformers for language modeling\n",
        "block_sz = 8\n",
        "x = train[:block_sz]\n",
        "y = train[1:block_sz + 1]\n",
        "for t in range(block_sz):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG-ZoDk53-aX",
        "outputId": "dff058b6-b741-4b5b-f608-1dd582ffeed6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(80)\n",
        "batch_sz = 4 #number of input sequences to be processed in parallel\n",
        "block_sz = 8 #maximum context length of predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate a small batch of data of inputs x and targets y\n",
        "  data = train if split == 'train' else val\n",
        "  ix = torch.randint(len(data) - block_sz, (batch_sz,))\n",
        "  x = torch.stack([data[i:i+block_sz] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_sz+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_sz): #batch dimension\n",
        "  for t in range(block_sz): #time dimension\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when the input is {context.tolist()}, the output is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z116RYtz75sa",
        "outputId": "26845435-bbb6-4cd0-beb7-342429629ef6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 42,  1, 42, 47, 42,  1, 58],\n",
            "        [39, 42, 50, 63,  1, 57, 54, 53],\n",
            "        [51,  1, 58, 53,  1, 39, 50, 50],\n",
            "        [63, 53, 59,  6,  1, 59, 52, 41]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[42,  1, 42, 47, 42,  1, 58, 56],\n",
            "        [42, 50, 63,  1, 57, 54, 53, 49],\n",
            "        [ 1, 58, 53,  1, 39, 50, 50, 11],\n",
            "        [53, 59,  6,  1, 59, 52, 41, 50]])\n",
            "-----\n",
            "when the input is [52], the output is 42\n",
            "when the input is [52, 42], the output is 1\n",
            "when the input is [52, 42, 1], the output is 42\n",
            "when the input is [52, 42, 1, 42], the output is 47\n",
            "when the input is [52, 42, 1, 42, 47], the output is 42\n",
            "when the input is [52, 42, 1, 42, 47, 42], the output is 1\n",
            "when the input is [52, 42, 1, 42, 47, 42, 1], the output is 58\n",
            "when the input is [52, 42, 1, 42, 47, 42, 1, 58], the output is 56\n",
            "when the input is [39], the output is 42\n",
            "when the input is [39, 42], the output is 50\n",
            "when the input is [39, 42, 50], the output is 63\n",
            "when the input is [39, 42, 50, 63], the output is 1\n",
            "when the input is [39, 42, 50, 63, 1], the output is 57\n",
            "when the input is [39, 42, 50, 63, 1, 57], the output is 54\n",
            "when the input is [39, 42, 50, 63, 1, 57, 54], the output is 53\n",
            "when the input is [39, 42, 50, 63, 1, 57, 54, 53], the output is 49\n",
            "when the input is [51], the output is 1\n",
            "when the input is [51, 1], the output is 58\n",
            "when the input is [51, 1, 58], the output is 53\n",
            "when the input is [51, 1, 58, 53], the output is 1\n",
            "when the input is [51, 1, 58, 53, 1], the output is 39\n",
            "when the input is [51, 1, 58, 53, 1, 39], the output is 50\n",
            "when the input is [51, 1, 58, 53, 1, 39, 50], the output is 50\n",
            "when the input is [51, 1, 58, 53, 1, 39, 50, 50], the output is 11\n",
            "when the input is [63], the output is 53\n",
            "when the input is [63, 53], the output is 59\n",
            "when the input is [63, 53, 59], the output is 6\n",
            "when the input is [63, 53, 59, 6], the output is 1\n",
            "when the input is [63, 53, 59, 6, 1], the output is 59\n",
            "when the input is [63, 53, 59, 6, 1, 59], the output is 52\n",
            "when the input is [63, 53, 59, 6, 1, 59, 52], the output is 41\n",
            "when the input is [63, 53, 59, 6, 1, 59, 52, 41], the output is 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEEDING OUR PREPROCESSED TRAINING DATA INTO NEURAL NETWORKS\n",
        "\n",
        "1. THE BI-GRAM LANGUAGE MODEL\n",
        "    A simple language model that predicts the next token in the sequence based on the single previous token. Can be extended to predict based on n preceding tokens (n-gram model)"
      ],
      "metadata": {
        "id": "548bb9aiMYh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(80)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_sz):\n",
        "      super().__init__()\n",
        "      #each token directly reads off the logits for the next token in a lookup table\n",
        "      self.token_embedding_table = nn.Embedding(vocab_sz, vocab_sz)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      #both idx and tensor are 2-dimensional (B,T) tensor of integers\n",
        "      logits = self.token_embedding_table(idx) #(B, T, C)\n",
        "\n",
        "      if targets == None:\n",
        "        loss = None\n",
        "      else:\n",
        "        #since pytorch expects a (B,C,T) tensor, we reshape the tensor using torch.view()\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T) #combining our batch and time dimensions\n",
        "\n",
        "        loss = F.cross_entropy(logits, targets) #negative log-likehood loss fn\n",
        "      return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "        # we pass our indexes to the forward fn to get our predictions(logits)\n",
        "        logits , loss = self(idx)\n",
        "        # focus on only the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B,C)\n",
        "        #convert predictions to class probabilities\n",
        "        probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "        # sample from the prob. distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        # append sampled index to running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_sz)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits)\n",
        "print(logits.shape)\n",
        "print(loss) #NOTE: ideal nll loss = -ln(1/vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih2olNgIAeta",
        "outputId": "9d9af26d-be48-4df4-8cc0-b23a0663e082"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2811, -0.2446,  0.3116,  ...,  0.2522, -0.0340, -0.8544],\n",
            "        [-0.0927, -1.2656, -2.1652,  ..., -0.0186,  0.9552,  1.3387],\n",
            "        [-0.3464, -1.1768, -2.6356,  ...,  1.7470,  1.1502, -0.0674],\n",
            "        ...,\n",
            "        [ 1.0463, -0.3701,  1.5730,  ...,  0.1144,  1.5161,  1.6939],\n",
            "        [ 1.2811, -0.2446,  0.3116,  ...,  0.2522, -0.0340, -0.8544],\n",
            "        [ 0.9166, -0.5317, -0.2654,  ...,  0.3093, -0.0615, -0.7909]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([32, 65])\n",
            "tensor(4.7332, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_seq = m.generate(torch.zeros((1,1), dtype= torch.long), max_new_tokens=100)[0].tolist() #generate a sequence of 100 characters using a newline character as initial idx\n",
        "print(decode(char_seq)) #prev op returns vaiid tokenizer idxs which we then decode"
      ],
      "metadata": {
        "id": "PJKVfuDctprx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172bfd3e-4448-4886-f9a0-80e22d67b6fd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "trlkNuJab BzxGdt,NgmnMN?EnoLqX&J:,frIWyz$z3:kbxZ;syPJlDAvcGnwkGZKOIwwd;3!gfyzq;cK!Vk\n",
            "mmG.!!Ia\n",
            "TTNda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create our optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "D86whd3iBjQ3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sz = 32\n",
        "for steps in range(10000):\n",
        "  #get a batch of data\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  #run through training loop\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "#print loss at tne end of training epochs\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FEm80gmBx-Y",
        "outputId": "bb462f3a-199c-4500-b7b8-16cc23cab0e4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3386425971984863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs after training\n",
        "char_seq = m.generate(torch.zeros((1,1), dtype= torch.long), max_new_tokens=100)[0].tolist() #generate a sequence of 100 characters using a newline character as initial idx\n",
        "print(decode(char_seq)) #prev op returns vaiid tokenizer idxs which we then decode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPoHdFX9EgcM",
        "outputId": "e2493534-20da-4fd2-957f-05b492a8b2ed"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INRGLIfitsamy ONGonthow wolld.\n",
            "Whours\n",
            "dsensse, thilisich his thehur ofar ANUMourouictl h g loorepewh\n"
          ]
        }
      ]
    }
  ]
}